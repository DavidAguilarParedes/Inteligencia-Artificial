{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTELIGENCIA ARTIFICIAL (INF371)¶\n",
    "\n",
    "Dr. Edwin Villanueva (evillatal@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratorio 6: Aprendizaje por Refuerzo  Q-learning  \n",
    "\n",
    "El presente laboratorio aborda la experimentacion de agentes de aprendizaje por refuerzo Q-learning en entornos grid. La implementacion del entorno PDM GridEnvironment y el agente Q-learning ya estan implementadas. Al final del notebook deberas responder a las preguntas planteadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Clase <b>GridEnvironment</b>\n",
    "\n",
    "La clase GridEnvironment define un entorno MDP (Proceso de Desiciones de Markov) para entornos grids (laberintos), como el ejemplo usado en clase. Las probabilidades de transicion son 0.8 para moverse en la dirección pretendida y 0.1 de moverse a un estado lateral. El constructor recibe:\n",
    "\n",
    "- grid: un array de listas de numeros definiendo los rewards del grid del entorno. Valores None indican un obstaculo\n",
    "- terminals: lista de estados terminales\n",
    "- initial: estado inicial\n",
    "- gamma: factor de descuento\n",
    "\n",
    "La clase mantiene el estado actual (current_state), el cual se inicializa en estado \"initial\" y se modifica con cada paso que se dé en el entorno (llamada a step()), devolviendo el nuevo estado, el reward y un flag 'done' que indica si el entorno ha caido en un estado terminal. El modelo de transicion de cada estado es accesible a travez de la funcion T(s,a) que devuelve una lista de tuplas (prob, s') para cada estado vecino s' del estado s ejecutando la accion a (prob es la probabilidad de transicionar de s a s' con accion a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "EAST, NORTH, WEST, SOUTH = (1, 0), (0, 1), (-1, 0), (0, -1)\n",
    "LEFT, RIGHT = +1, -1\n",
    "        \n",
    "class GridEnvironment:\n",
    "    def __init__(self, grid, terminals, initial=(0, 0), gamma=.99):\n",
    "        grid.reverse()     # para que fila 0 sea la de abajo, no la de arriba\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "        self.grid = grid\n",
    "        self.initial_state = initial\n",
    "        self.current_state = initial\n",
    "        self.terminals = terminals\n",
    "        self.gamma = gamma\n",
    "        self.actionlist = [EAST, NORTH, WEST, SOUTH] \n",
    "\n",
    "        self.rewards = {}        # diccionario de rewards\n",
    "        self.states = set()     # conjunto de estados diferentes\n",
    "        for x in range(self.cols):   # obtiene todos los estados y rewards del grid\n",
    "            for y in range(self.rows):\n",
    "                if grid[y][x]:  # Si la celda no es None (Prohibida), agrega el estado y reward\n",
    "                    self.states.add((x, y))\n",
    "                    self.rewards[(x, y)] = grid[y][x]\n",
    "            \n",
    "        self.transition_probs = {}  # almacena los diccionarios de probabilidades de transicion\n",
    "        for s in self.states:\n",
    "            self.transition_probs[s] = {}  # diccionario de probabilidades de transicion de los vecinos de estado s\n",
    "            for a in self.actionlist:\n",
    "                self.transition_probs[s][a] = self.get_transition_probs(s, a)\n",
    "                \n",
    "    def get_transition_probs(self, state, action): \n",
    "        # Hay 0.8 de probabilidad de moverse en la dirección pretendida y 0.1 de moverse por cada lateral. \n",
    "        if action:\n",
    "            return [(0.8, self.go(state, action)),\n",
    "                    (0.1, self.go(state, self.turn_right(action))),\n",
    "                    (0.1, self.go(state, self.turn_left(action)))]\n",
    "        else:\n",
    "            return [(0.0, state)]\n",
    "        \n",
    "    def go(self, state, direction):\n",
    "        \"\"\"Retorna el estado que resultaria de ir en la direccion pasada, si el ambiente fuese deterministico \"\"\"\n",
    "        state1 = tuple(map(operator.add, state, direction))\n",
    "        return state1 if state1 in self.states else state    \n",
    "    \n",
    "    def turn_heading(self, heading, inc, headings=[EAST, NORTH, WEST, SOUTH]):\n",
    "        return headings[(headings.index(heading) + inc) % len(headings)]\n",
    "\n",
    "    def turn_right(self, heading):\n",
    "        return self.turn_heading(heading, RIGHT)\n",
    "\n",
    "    def turn_left(self, heading):\n",
    "        return self.turn_heading(heading, LEFT) \n",
    "    \n",
    "    def T(self, s, a):  # Retorna los estados vecinos y sus prob de transicion, tuplas (prob, s'), para el estado  s y accion a\n",
    "        return self.transition_probs[s][a] if a else [(0.0, s)]\n",
    "\n",
    "    def R(self, state): # retorna el reward de un estado\n",
    "        return self.rewards[state]    \n",
    "    \n",
    "    def actions(self, state): # retorna la lista de acciones posibles en un estado \n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actionlist    \n",
    "    \n",
    "    def reset(self):  # Reseta el Entorno\n",
    "        self.current_state = self.initial_state\n",
    "        return self.current_state, self.rewards[self.current_state]\n",
    "    \n",
    "    def step(self, action): # Ejecuta un paso el entorno. Retorna el nuevo estado, el reward y flag de que es estado terminal\n",
    "        x = random.uniform(0, 1)\n",
    "        cumulative_probability = 0.0\n",
    "        for probability_state in self.T(self.current_state, action):\n",
    "            probability, next_state = probability_state\n",
    "            cumulative_probability += probability\n",
    "            if x < cumulative_probability:\n",
    "                break\n",
    "        self.current_state = next_state\n",
    "        done = True if current_state in self.terminals else False\n",
    "        return self.current_state, self.rewards[self.current_state], done\n",
    "    \n",
    "    def to_grid(self, mapping):\n",
    "        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\n",
    "        return list(reversed([[mapping.get((x, y), None)\n",
    "                               for x in range(self.cols)]\n",
    "                               for y in range(self.rows)]))\n",
    "\n",
    "    def to_arrows(self, policy):\n",
    "        chars = {(1, 0): '>', (0, 1): '^', (-1, 0): '<', (0, -1): 'v', None: '.'}\n",
    "        return self.to_grid({s: chars[a] for (s, a) in policy.items()})\n",
    "    \n",
    "    def print_policy(self, policy):\n",
    "        \"\"\"Imprime la politica\"\"\"\n",
    "        header=None\n",
    "        sep='   '\n",
    "        numfmt='{}'\n",
    "        table = self.to_arrows(policy)\n",
    "        justs = ['rjust' if hasattr(x, '__int__') else 'ljust' for x in table[0]]\n",
    "\n",
    "        if header:\n",
    "            table.insert(0, header)\n",
    "\n",
    "        table = [[numfmt.format(x) if hasattr(x, '__int__') else x for x in row]\n",
    "                 for row in table]\n",
    "\n",
    "        sizes = list(\n",
    "            map(lambda seq: max(map(len, seq)),\n",
    "                list(zip(*[map(str, row) for row in table]))))\n",
    "\n",
    "        for row in table:\n",
    "            print(sep.join(getattr(\n",
    "                str(x), j)(size) for (j, size, x) in zip(justs, sizes, row)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Entorno para experimentar </b>\n",
    "Para experimentar,  se usará el entorno MDP definido abajo. El factor de descuento es $\\gamma = 0.99$ (en los ejemplos de clase se usó $\\gamma = 1$). Las recompensas son **-0.1** en estados no terminales y **+5** y **-5** en estados terminales.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el grid que se vio en clase\n",
    "#grid = [[-0.04, -0.04, -0.04, +1],\n",
    "#        [-0.04,  None, -0.04, -1],\n",
    "#        [-0.04, -0.04, -0.04, -0.04]]\n",
    "\n",
    "# el grid de este desafio\n",
    "grid = [\n",
    "    [None, None, None, None, None, None, None, None, None, None, None], \n",
    "    [None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None, +5.0, None], \n",
    "    [None, -0.1, None, None, None, None, None, None, None, -0.1, None], \n",
    "    [None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None], \n",
    "    [None, -0.1, None, None, None, None, None, None, None, None, None], \n",
    "    [None, -0.1, None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None], \n",
    "    [None, -0.1, None, None, None, None, None, -0.1, None, -0.1, None], \n",
    "    [None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None, -0.1, None], \n",
    "    [None, None, None, None, None, -0.1, None, -0.1, None, -0.1, None], \n",
    "    [None, -5.0, -0.1, -0.1, -0.1, -0.1, None, -0.1, None, -0.1, None], \n",
    "    [None, None, None, None, None, None, None, None, None, None, None]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase <b>QLearningAgent</b>\n",
    "\n",
    "Esta clase define un agente exploratorio Q-learning. Este evita aprender el modelo de transicion ya que los Q-valores de un estado-action puede ser relacionado directamente a los Q-valores de los estado-action vecinos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \n",
    "    def __init__(self, mdp, Ne, Rplus, alpha=None):\n",
    "\n",
    "        self.gamma = mdp.gamma    # factor de descuento (definido en el MDP)\n",
    "        self.terminals = mdp.terminals   # estados terminales (definido en el MDP)\n",
    "        self.all_act = mdp.actionlist  # acciones posibles\n",
    "        self.Ne = Ne        # limite de iteraciones de la funcion de exploracion\n",
    "        self.Rplus = Rplus  # Recompensa que tienen los estados (o q-estados) antes del limite de iteraciones Ne\n",
    "        self.Q = defaultdict(float)   # almacena los q-valores\n",
    "        self.Nsa = defaultdict(float) # almacena la tabla de frecuencias state-action\n",
    "        self.s = None    # estado anterior\n",
    "        self.a = None    # ultima accion ejecutada\n",
    "        self.r = None    # recompensa de estado anterior\n",
    "\n",
    "        if alpha:\n",
    "            self.alpha = alpha   # alpha es la taza de aprendizaje. Debe disminuir con el numero de visitas al estado para que las utilidades converjan\n",
    "        else:\n",
    "            self.alpha = lambda n: 1./(1+n)  # udacity video\n",
    "\n",
    "    def f(self, u, n): \n",
    "        \"\"\" Funcion de exploracion. Retorna un valor de utilidad fijo (Rplus) hasta que el agente visita Ne veces el state-action \"\"\"\n",
    "        if n < self.Ne:\n",
    "            return self.Rplus\n",
    "        else:\n",
    "            return u\n",
    "\n",
    "    def actions_in_state(self, state):\n",
    "        \"\"\" Retorna el conbjunto de acciones posibles del estado pasado. Util para max y argmax. \"\"\"\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.all_act\n",
    "\n",
    "    # Programa del agente Q-learning    \n",
    "    def __call__(self, percept):    \n",
    "        \"\"\" Este es el programa del agente que es llamado en cada step, recibe un percept y retorna una accion \"\"\"\n",
    "        s1, r1 = self.update_state(percept)\n",
    "        Q, Nsa, s, a, r = self.Q, self.Nsa, self.s, self.a, self.r\n",
    "        alpha, gamma, terminals = self.alpha, self.gamma, self.terminals,\n",
    "        actions_in_state = self.actions_in_state\n",
    "\n",
    "        if s in terminals:\n",
    "            Q[s, None] = r1\n",
    "        if s is not None:\n",
    "            Nsa[s, a] += 1\n",
    "            Q[s, a] += alpha(Nsa[s, a]) * (r + gamma * max(Q[s1, a1] for a1 in actions_in_state(s1)) - Q[s, a])\n",
    "        if s in terminals:\n",
    "            self.s = self.a = self.r = None\n",
    "        else:\n",
    "            self.s, self.r = s1, r1\n",
    "            self.a = max(actions_in_state(s1), key=lambda a1: self.f(Q[s1, a1], Nsa[s1, a1])) # funciona como argmax, devuelve la accion con mayor f\n",
    "        return self.a\n",
    "\n",
    "    def update_state(self, percept):\n",
    "        ''' To be overridden in most cases. The default case\n",
    "        assumes the percept to be of type (state, reward)'''\n",
    "        return percept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando el agente  <b>Q-learning</b>\n",
    "\n",
    "Vamos a instanciar un agente Q-learning para aprender una politica en nuestro entorno de prueba \"grid\". Los parametros del agente son los siguientes: **Ne = 5**, **Rplus = 2**, **alpha** como dado en la nota de pie del libro **pagina 837**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia el entorno del grid\n",
    "#environment = GridEnvironment(grid, terminals=[(3, 2), (3, 1)], initial=(0, 0), gamma=0.99) # grid de la clase\n",
    "environment = GridEnvironment(grid, terminals=[(1, 1), (9, 9)], initial=(3, 1), gamma=0.99) # grid de este laboratorio\n",
    "\n",
    "# Instancia un agente Q-learning \n",
    "agent = QLearningAgent(environment, Ne=5, Rplus=10, alpha=lambda n: 60./(59+n)) \n",
    "\n",
    "# Ejecuta 10000 episodios del agente en el entorno\n",
    "TRIALS = 10000      \n",
    "for e in range(TRIALS):   # Por caa trial\n",
    "    current_state, current_reward = environment.reset()\n",
    "    score_trial = current_reward   # el escore del episodio es la suma acumulada de rewards en el episodio \n",
    "    while True:  # ejecuta steps del entorno hasta llegar a un estado terminal\n",
    "        percept = (current_state, current_reward)  # la percepcion del agente es la tupla (state, reward)\n",
    "        action  = agent(percept)  # llama al programa del agente, pasandole el percept y espera una accion a ejecutar\n",
    "        current_state, current_reward, done = environment.step(action) # ejecuta la accion en el entorno, \n",
    "        score_trial += current_reward\n",
    "        if done:\n",
    "            print(\"Trial: {}/{}, score: {}\".format(e, TRIALS, score_trial))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos el diccionario de los Q-valores aprendidos. Las claves son pares state-action. Las diferentes acciones corresponden a:\n",
    "\n",
    "NORTH = (0, 1)  \n",
    "SOUTH = (0,-1)  \n",
    "WEST = (-1, 0)  \n",
    "EAST = (1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qvalues = agent.Q\n",
    "#print(Qvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PREGUNTAS:\n",
    "\n",
    "\n",
    "<b>1) Cree una funcion para extraer las utilidades (U) de los estados a partir de los Q-valores obtenidos por el agente. LLame esta funcion: get_utilities_from_qvalues(Qvalues). Pruebela en el resultado anterior (agent.Q)</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utilities_from_qvalues(mdp, Q):\n",
    "    \"\"\"Dado un MDP y una funcion de utilidad Q, determina los valores de utilidad de los estados. \"\"\"\n",
    "    U = {}\n",
    "    for s in mdp.states:\n",
    "        if s not in mdp.terminals:\n",
    "            U[s] =  -np.inf\n",
    "            for a in mdp.actionlist:\n",
    "                if Q[(s, a)] > U[s] : \n",
    "                    U[s] = Q[(s, a)]\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_qlearning = get_utilities_from_qvalues(environment, agent.Q)\n",
    "print(U_qlearning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>2)  Cree una funcion para extraer la politica a partir de los Q-valores obtenidos por el agente. LLame esta funcion: get_policy_from_qvalues(Qvalues). Pruebela en el resultado anterior (agent.Q)</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_from_qvalues(mdp, Q):\n",
    "    \"\"\"Dado un MDP y una funccion de utilidad Q, determina la mejor politica. \"\"\"\n",
    "    pi = {}\n",
    "    for s in mdp.states:\n",
    "        if s not in mdp.terminals:\n",
    "            pi[s] = max(mdp.actionlist, key=lambda a: Q[(s,a)])\n",
    "        else:\n",
    "            pi[s] = None\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_qlearning = get_policy_from_qvalues(environment, Qvalues)\n",
    "environment.print_policy(pi_qlearning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 3) Cree una función para comparar dos politicas dadas y devolver el numero de estados en que NO COINCIDEN las politicas. Llame dicha funcion: comparare_policies(policy1, policy2). Pruebe la función con:  <b>\n",
    "    \n",
    "    - policy1 = la política optima de resolver el MDP con el metodo value_iteration (pi_valueiteration)\n",
    "    - policy2 = la politica obtenida con Q-learning en la pregunta 2  (pi_qlearning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener la politica optima con el metodo value_iteration puede ejecutar el siguiente codigo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import *\n",
    "U_valueiteration = value_iteration(environment, .001)  # encuentra las utilidades de los estados con value_iteration\n",
    "pi_valueiteration = best_policy(environment, U_valueiteration)  # obtiene la politica optima de las utilidades encontradas\n",
    "environment.print_policy(pi_valueiteration)   # imprime la politica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (U_valueiteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparare_policies(policy1, policy2):\n",
    "    \n",
    "    # Escribir su codigo aqui\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_acciones_diferentes = comparare_policies(pi_valueiteration, pi_qlearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_acciones_diferentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4) Para cada valor siguiente de Ne: {0, 1, 5, 10, 100}  ejecute el agente unas 10 veces (10 veces el codigo de abajo), cada vez comparando la politica obtenida por el agente Q-learning con la politica óptima de Value Iteration (use la funcion que implementó comparare_policies() ). Registre la media del numero de acciones diferentes para cada valor de Ne. Cuál valor de Ne genera resultados mas proximos a la politica optima? Intente dar una justificación al respecto relacionando las características del entorno y la teoría </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intancia entorno\n",
    "environment = GridEnvironment(grid, terminals=[(1, 1), (9, 9)], initial=(3, 1), gamma=0.99) # \n",
    "\n",
    "# Instancia un agente Q-learning \n",
    "agent = QLearningAgent(environment, Ne, Rplus=5, alpha=lambda n: 60./(59+n)) \n",
    "\n",
    "# Ejecuta 10000 episodios del agente en el entorno\n",
    "TRIALS = 10000      \n",
    "for e in range(TRIALS):   # Por caa trial\n",
    "    current_state, current_reward = environment.reset()\n",
    "    score_trial = current_reward   # el escore del episodio es la suma acumulada de rewards en el episodio \n",
    "    while True:  # ejecuta steps del entorno hasta llegar a un estado terminal\n",
    "        percept = (current_state, current_reward)  # la percepcion del agente es la tupla (state, reward)\n",
    "        action  = agent(percept)  # llama al programa del agente, pasandole el percept y espera una accion a ejecutar\n",
    "        current_state, current_reward, done = environment.step(action) # ejecuta la accion en el entorno, \n",
    "        score_trial += current_reward\n",
    "        if done:\n",
    "            #print(\"Trial: {}/{}, score: {}\".format(e, TRIALS, score_trial))\n",
    "            break\n",
    "            \n",
    "pi_qlearning = get_policy_from_qvalues(environment, agent.Q)\n",
    "num_acciones_diferentes = comparare_policies(pi_valueiteration, pi_qlearning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 5) Teoricamente, Qué efecto tiene el parametro gamma en el aprendizaje de q-learning </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 6) Si la política encontrada por Value_iteration es óptima, Por qué no se puede usar siempre dicho método para encontrar la política? </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
