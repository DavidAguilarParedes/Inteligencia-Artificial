{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTELIGENCIA ARTIFICIAL (INF371)¶\n",
    "\n",
    "Dr. Edwin Villanueva (evillatal@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje por refuerzo con Deep Q-networks  - juego CartPole\n",
    "\n",
    "Esta es una implementacion de un agente Deep Q-learning que aprende a equilibrar el mastil del juego CartPole. Adaptado de  https://github.com/keon/deep-q-learning  \n",
    "\n",
    "Es necesario tener instalado  gym, tensorflow y keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase <b>DQNAgent</b>\n",
    "\n",
    "Esta es la clase que implementa el agente Deep Q-lerrning. Es una implementacion general, pudiendo ser usado en diferentes entornos de gym u otros. El constructor recibe las dimensiones del estado y la dimesion del vector de acciones posibles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size     # tamaño de un estado (numero de atributos que representan un estado)\n",
    "        self.action_size = action_size   # tamaño del vector de acciones \n",
    "        self.memory = deque(maxlen=3000)  # define la memoria del agente (2000 registros como maximo)\n",
    "        self.gamma = 0.95                 # discount rate\n",
    "        self.learning_rate = 0.001        # taza de aprendizaje \n",
    "        \n",
    "        self.epsilon = 1.0          # factor de exploration inicial\n",
    "        self.epsilon_min = 0.01     # factor de exploration minimo\n",
    "        self.epsilon_decay = 0.995   # factor de decaimiento del factor de exploracion\n",
    "        self.model = self._build_model()  # construye el modelo neuronal para estimar las utilidades\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Define y compila un modelo de red neuronal de 3 capas: state_size entradas X 20 neuronas X 20 neuronas x action_size neuronas de salida\n",
    "        model = Sequential()   # Informa que las capas que se van agregar son secuenciales\n",
    "        model.add(Dense(20, input_dim=self.state_size, activation='relu')) # 1ra capa de 20 neuronas, cada neurona recibe state_size entradas (4 para CartPole), activacion relu\n",
    "        model.add(Dense(20, activation='relu')) # 2da capa de 20 neuronas, funcion de activacion relu\n",
    "        model.add(Dense(self.action_size, activation='linear')) # 3ra capa (salida) de action_size neuronas (2 para CartPole)\n",
    "       \n",
    "        model.compile(loss='mse', optimizer = Adam(lr=self.learning_rate)) # la funcion de perdida es el error cuadratico medio (mse)\n",
    "        return model\n",
    "\n",
    "    # metodo para guardar una transicion del agente (experiencia): (estado, accion, reward resultante, nuevo estado, done)\n",
    "    # done es un flag que indica que el entorno cayo en un estado terminal\n",
    "    def remember(self, state, action, reward, next_state, done): \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # retorna una accion.  \n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:  # retorna una accion aleatoria con probabilidad self.epsilon\n",
    "            return random.randrange(self.action_size)\n",
    "        action_values = self.model.predict(state) # obtiene los q valores predichos por el modelo para cada accion\n",
    "        return np.argmax(action_values[0])  # retorna la accion con el maximo q-valor predicho\n",
    "\n",
    "    def replay(self, batch_size): # ajusta la red neuronal con una muestra de su memoria de tamaño batch_size\n",
    "        # obtiene una muestra de su memoria de experiencias\n",
    "        minibatch = random.sample(self.memory, batch_size) \n",
    "        \n",
    "        # recorre cada experiencia del minibatch de experiencias\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            \n",
    "            # target es el vector de Q values de las posibles acciones desde state (por defecto son los predichos por el modelo)\n",
    "            target = self.model.predict(state)\n",
    "            \n",
    "            if done:  # si cayo en un estado terminal\n",
    "                # Actualiza el Q valor del target correspondiente a action, colocando el valor Q = reward\n",
    "                target[0][action] = reward   \n",
    "            else:  # si  no es estado terminal \n",
    "                # Predice los valores Q del next_state usando el modelo\n",
    "                Qvals_next_state = self.model.predict(next_state)[0]\n",
    "                # Actualiza el Q value del target correspondiente a la accion action con el future discounted reward\n",
    "                target[0][action] = reward + self.gamma * np.amax(Qvals_next_state)\n",
    " \n",
    "            self.model.fit(state, target, epochs=1, verbose=0) # ajusta pesos de la red con el ejemplo: (state,target)\n",
    "\n",
    "        # si no esta en el valor minimo del factor de exploracion -> hace un decaimiento del factor de exploracion\n",
    "        if self.epsilon > self.epsilon_min: \n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutando el agente de aprendizaje DQN en el entorno CartPole\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "episode: 0/500, score: 32, e: 1.0\n",
      "episode: 1/500, score: 15, e: 1.0\n",
      "episode: 2/500, score: 21, e: 1.0\n",
      "episode: 3/500, score: 10, e: 1.0\n",
      "episode: 4/500, score: 10, e: 1.0\n",
      "episode: 5/500, score: 32, e: 1.0\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "episode: 6/500, score: 13, e: 0.99\n",
      "episode: 7/500, score: 8, e: 0.99\n",
      "episode: 8/500, score: 14, e: 0.99\n",
      "episode: 9/500, score: 13, e: 0.98\n",
      "episode: 10/500, score: 15, e: 0.98\n",
      "episode: 11/500, score: 13, e: 0.97\n",
      "episode: 12/500, score: 13, e: 0.97\n",
      "episode: 13/500, score: 8, e: 0.96\n",
      "episode: 14/500, score: 23, e: 0.96\n",
      "episode: 15/500, score: 16, e: 0.95\n",
      "episode: 16/500, score: 35, e: 0.95\n",
      "episode: 17/500, score: 17, e: 0.94\n",
      "episode: 18/500, score: 25, e: 0.94\n",
      "episode: 19/500, score: 10, e: 0.93\n",
      "episode: 20/500, score: 16, e: 0.93\n",
      "episode: 21/500, score: 25, e: 0.92\n",
      "episode: 22/500, score: 12, e: 0.92\n",
      "episode: 23/500, score: 17, e: 0.91\n",
      "episode: 24/500, score: 10, e: 0.91\n",
      "episode: 25/500, score: 22, e: 0.9\n",
      "episode: 26/500, score: 49, e: 0.9\n",
      "episode: 27/500, score: 19, e: 0.9\n",
      "episode: 28/500, score: 54, e: 0.89\n",
      "episode: 29/500, score: 26, e: 0.89\n",
      "episode: 30/500, score: 18, e: 0.88\n",
      "episode: 31/500, score: 15, e: 0.88\n",
      "episode: 32/500, score: 20, e: 0.87\n",
      "episode: 33/500, score: 17, e: 0.87\n",
      "episode: 34/500, score: 17, e: 0.86\n",
      "episode: 35/500, score: 15, e: 0.86\n",
      "episode: 36/500, score: 9, e: 0.86\n",
      "episode: 37/500, score: 28, e: 0.85\n",
      "episode: 38/500, score: 59, e: 0.85\n",
      "episode: 39/500, score: 39, e: 0.84\n",
      "episode: 40/500, score: 26, e: 0.84\n",
      "episode: 41/500, score: 26, e: 0.83\n",
      "episode: 42/500, score: 53, e: 0.83\n",
      "episode: 43/500, score: 15, e: 0.83\n",
      "episode: 44/500, score: 13, e: 0.82\n",
      "episode: 45/500, score: 17, e: 0.82\n",
      "episode: 46/500, score: 45, e: 0.81\n",
      "episode: 47/500, score: 10, e: 0.81\n",
      "episode: 48/500, score: 9, e: 0.81\n",
      "episode: 49/500, score: 10, e: 0.8\n",
      "episode: 50/500, score: 25, e: 0.8\n",
      "episode: 51/500, score: 17, e: 0.79\n",
      "episode: 52/500, score: 15, e: 0.79\n",
      "episode: 53/500, score: 14, e: 0.79\n",
      "episode: 54/500, score: 24, e: 0.78\n",
      "episode: 55/500, score: 25, e: 0.78\n",
      "episode: 56/500, score: 41, e: 0.77\n",
      "episode: 57/500, score: 46, e: 0.77\n",
      "episode: 58/500, score: 23, e: 0.77\n",
      "episode: 59/500, score: 22, e: 0.76\n",
      "episode: 60/500, score: 45, e: 0.76\n",
      "episode: 61/500, score: 32, e: 0.76\n",
      "episode: 62/500, score: 72, e: 0.75\n",
      "episode: 63/500, score: 78, e: 0.75\n",
      "episode: 64/500, score: 35, e: 0.74\n",
      "episode: 65/500, score: 18, e: 0.74\n",
      "episode: 66/500, score: 68, e: 0.74\n",
      "episode: 67/500, score: 34, e: 0.73\n",
      "episode: 68/500, score: 29, e: 0.73\n",
      "episode: 69/500, score: 41, e: 0.73\n",
      "episode: 70/500, score: 19, e: 0.72\n",
      "episode: 71/500, score: 21, e: 0.72\n",
      "episode: 72/500, score: 24, e: 0.71\n",
      "episode: 73/500, score: 19, e: 0.71\n",
      "episode: 74/500, score: 24, e: 0.71\n",
      "episode: 75/500, score: 50, e: 0.7\n",
      "episode: 76/500, score: 62, e: 0.7\n",
      "episode: 77/500, score: 14, e: 0.7\n",
      "episode: 78/500, score: 14, e: 0.69\n",
      "episode: 79/500, score: 37, e: 0.69\n",
      "episode: 80/500, score: 34, e: 0.69\n",
      "episode: 81/500, score: 24, e: 0.68\n",
      "episode: 82/500, score: 51, e: 0.68\n",
      "episode: 83/500, score: 97, e: 0.68\n",
      "episode: 84/500, score: 157, e: 0.67\n",
      "episode: 85/500, score: 34, e: 0.67\n",
      "episode: 86/500, score: 10, e: 0.67\n",
      "episode: 87/500, score: 44, e: 0.66\n",
      "episode: 88/500, score: 31, e: 0.66\n",
      "episode: 89/500, score: 63, e: 0.66\n",
      "episode: 90/500, score: 105, e: 0.65\n",
      "episode: 91/500, score: 57, e: 0.65\n",
      "episode: 92/500, score: 35, e: 0.65\n",
      "episode: 93/500, score: 44, e: 0.64\n",
      "episode: 94/500, score: 24, e: 0.64\n",
      "episode: 95/500, score: 73, e: 0.64\n",
      "episode: 96/500, score: 60, e: 0.63\n",
      "episode: 97/500, score: 34, e: 0.63\n",
      "episode: 98/500, score: 62, e: 0.63\n",
      "episode: 99/500, score: 29, e: 0.62\n",
      "episode: 100/500, score: 42, e: 0.62\n",
      "episode: 101/500, score: 86, e: 0.62\n",
      "episode: 102/500, score: 84, e: 0.61\n",
      "episode: 103/500, score: 45, e: 0.61\n",
      "episode: 104/500, score: 63, e: 0.61\n",
      "episode: 105/500, score: 20, e: 0.61\n",
      "episode: 106/500, score: 79, e: 0.6\n",
      "episode: 107/500, score: 53, e: 0.6\n",
      "episode: 108/500, score: 62, e: 0.6\n",
      "episode: 109/500, score: 130, e: 0.59\n",
      "episode: 110/500, score: 127, e: 0.59\n",
      "episode: 111/500, score: 29, e: 0.59\n",
      "episode: 112/500, score: 91, e: 0.58\n",
      "episode: 113/500, score: 70, e: 0.58\n",
      "episode: 114/500, score: 94, e: 0.58\n",
      "episode: 115/500, score: 53, e: 0.58\n",
      "episode: 116/500, score: 72, e: 0.57\n",
      "episode: 117/500, score: 49, e: 0.57\n",
      "episode: 118/500, score: 19, e: 0.57\n",
      "episode: 119/500, score: 110, e: 0.56\n",
      "episode: 120/500, score: 76, e: 0.56\n",
      "episode: 121/500, score: 11, e: 0.56\n",
      "episode: 122/500, score: 107, e: 0.56\n",
      "episode: 123/500, score: 104, e: 0.55\n",
      "episode: 124/500, score: 13, e: 0.55\n",
      "episode: 125/500, score: 98, e: 0.55\n",
      "episode: 126/500, score: 72, e: 0.55\n",
      "episode: 127/500, score: 102, e: 0.54\n",
      "episode: 128/500, score: 100, e: 0.54\n",
      "episode: 129/500, score: 108, e: 0.54\n",
      "episode: 130/500, score: 122, e: 0.53\n",
      "episode: 131/500, score: 45, e: 0.53\n",
      "episode: 132/500, score: 94, e: 0.53\n",
      "episode: 133/500, score: 90, e: 0.53\n",
      "episode: 134/500, score: 43, e: 0.52\n",
      "episode: 135/500, score: 107, e: 0.52\n",
      "episode: 136/500, score: 90, e: 0.52\n",
      "episode: 137/500, score: 24, e: 0.52\n",
      "episode: 138/500, score: 124, e: 0.51\n",
      "episode: 139/500, score: 49, e: 0.51\n",
      "episode: 140/500, score: 17, e: 0.51\n",
      "episode: 141/500, score: 34, e: 0.51\n",
      "episode: 142/500, score: 115, e: 0.5\n",
      "episode: 143/500, score: 80, e: 0.5\n",
      "episode: 144/500, score: 26, e: 0.5\n",
      "episode: 145/500, score: 204, e: 0.5\n",
      "episode: 146/500, score: 40, e: 0.49\n",
      "episode: 147/500, score: 127, e: 0.49\n",
      "episode: 148/500, score: 51, e: 0.49\n",
      "episode: 149/500, score: 26, e: 0.49\n",
      "episode: 150/500, score: 117, e: 0.48\n",
      "episode: 151/500, score: 170, e: 0.48\n",
      "episode: 152/500, score: 48, e: 0.48\n",
      "episode: 153/500, score: 116, e: 0.48\n",
      "episode: 154/500, score: 19, e: 0.47\n",
      "episode: 155/500, score: 29, e: 0.47\n",
      "episode: 156/500, score: 53, e: 0.47\n",
      "episode: 157/500, score: 92, e: 0.47\n",
      "episode: 158/500, score: 183, e: 0.46\n",
      "episode: 159/500, score: 117, e: 0.46\n",
      "episode: 160/500, score: 104, e: 0.46\n",
      "episode: 161/500, score: 17, e: 0.46\n",
      "episode: 162/500, score: 91, e: 0.46\n",
      "episode: 163/500, score: 171, e: 0.45\n",
      "episode: 164/500, score: 145, e: 0.45\n",
      "episode: 165/500, score: 228, e: 0.45\n",
      "episode: 166/500, score: 58, e: 0.45\n",
      "episode: 167/500, score: 124, e: 0.44\n",
      "episode: 168/500, score: 23, e: 0.44\n",
      "episode: 169/500, score: 127, e: 0.44\n",
      "episode: 170/500, score: 117, e: 0.44\n",
      "episode: 171/500, score: 122, e: 0.44\n",
      "episode: 172/500, score: 374, e: 0.43\n",
      "episode: 173/500, score: 147, e: 0.43\n",
      "episode: 174/500, score: 122, e: 0.43\n",
      "episode: 175/500, score: 108, e: 0.43\n",
      "episode: 176/500, score: 163, e: 0.42\n",
      "episode: 177/500, score: 155, e: 0.42\n",
      "episode: 178/500, score: 78, e: 0.42\n",
      "episode: 179/500, score: 137, e: 0.42\n",
      "episode: 180/500, score: 158, e: 0.42\n",
      "episode: 181/500, score: 163, e: 0.41\n",
      "episode: 182/500, score: 240, e: 0.41\n",
      "episode: 183/500, score: 182, e: 0.41\n",
      "episode: 184/500, score: 144, e: 0.41\n",
      "episode: 185/500, score: 153, e: 0.41\n",
      "episode: 186/500, score: 220, e: 0.4\n",
      "episode: 187/500, score: 188, e: 0.4\n",
      "episode: 188/500, score: 182, e: 0.4\n",
      "episode: 189/500, score: 197, e: 0.4\n",
      "episode: 190/500, score: 220, e: 0.4\n",
      "episode: 191/500, score: 170, e: 0.39\n",
      "episode: 192/500, score: 17, e: 0.39\n",
      "episode: 193/500, score: 132, e: 0.39\n",
      "episode: 194/500, score: 167, e: 0.39\n",
      "episode: 195/500, score: 206, e: 0.39\n",
      "episode: 196/500, score: 216, e: 0.38\n",
      "episode: 197/500, score: 199, e: 0.38\n",
      "episode: 198/500, score: 182, e: 0.38\n",
      "episode: 199/500, score: 47, e: 0.38\n",
      "episode: 200/500, score: 128, e: 0.38\n",
      "episode: 201/500, score: 195, e: 0.37\n",
      "episode: 202/500, score: 129, e: 0.37\n",
      "episode: 203/500, score: 243, e: 0.37\n",
      "episode: 204/500, score: 190, e: 0.37\n",
      "episode: 205/500, score: 232, e: 0.37\n",
      "episode: 206/500, score: 158, e: 0.37\n",
      "episode: 207/500, score: 256, e: 0.36\n",
      "episode: 208/500, score: 25, e: 0.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 209/500, score: 162, e: 0.36\n",
      "episode: 210/500, score: 163, e: 0.36\n",
      "episode: 211/500, score: 244, e: 0.36\n",
      "episode: 212/500, score: 170, e: 0.35\n",
      "episode: 213/500, score: 248, e: 0.35\n",
      "episode: 214/500, score: 145, e: 0.35\n",
      "episode: 215/500, score: 191, e: 0.35\n",
      "episode: 216/500, score: 151, e: 0.35\n",
      "episode: 217/500, score: 163, e: 0.35\n",
      "episode: 218/500, score: 194, e: 0.34\n",
      "episode: 219/500, score: 185, e: 0.34\n",
      "episode: 220/500, score: 189, e: 0.34\n",
      "episode: 221/500, score: 218, e: 0.34\n",
      "episode: 222/500, score: 34, e: 0.34\n",
      "episode: 223/500, score: 195, e: 0.34\n",
      "episode: 224/500, score: 43, e: 0.33\n",
      "episode: 225/500, score: 298, e: 0.33\n",
      "episode: 226/500, score: 355, e: 0.33\n",
      "episode: 227/500, score: 499, e: 0.33\n",
      "episode: 228/500, score: 284, e: 0.33\n",
      "episode: 229/500, score: 233, e: 0.33\n",
      "episode: 230/500, score: 194, e: 0.32\n",
      "episode: 231/500, score: 162, e: 0.32\n",
      "episode: 232/500, score: 172, e: 0.32\n",
      "episode: 233/500, score: 227, e: 0.32\n",
      "episode: 234/500, score: 208, e: 0.32\n",
      "episode: 235/500, score: 179, e: 0.32\n",
      "episode: 236/500, score: 196, e: 0.31\n",
      "episode: 237/500, score: 168, e: 0.31\n",
      "episode: 238/500, score: 268, e: 0.31\n",
      "episode: 239/500, score: 200, e: 0.31\n",
      "episode: 240/500, score: 258, e: 0.31\n",
      "episode: 241/500, score: 169, e: 0.31\n",
      "episode: 242/500, score: 188, e: 0.3\n",
      "episode: 243/500, score: 149, e: 0.3\n",
      "episode: 244/500, score: 159, e: 0.3\n",
      "episode: 245/500, score: 133, e: 0.3\n",
      "episode: 246/500, score: 156, e: 0.3\n",
      "episode: 247/500, score: 100, e: 0.3\n",
      "episode: 248/500, score: 165, e: 0.3\n",
      "episode: 249/500, score: 187, e: 0.29\n",
      "episode: 250/500, score: 147, e: 0.29\n",
      "episode: 251/500, score: 122, e: 0.29\n",
      "episode: 252/500, score: 151, e: 0.29\n",
      "episode: 253/500, score: 160, e: 0.29\n",
      "episode: 254/500, score: 186, e: 0.29\n",
      "episode: 255/500, score: 145, e: 0.29\n",
      "episode: 256/500, score: 158, e: 0.28\n",
      "episode: 257/500, score: 178, e: 0.28\n",
      "episode: 258/500, score: 127, e: 0.28\n",
      "episode: 259/500, score: 149, e: 0.28\n",
      "episode: 260/500, score: 102, e: 0.28\n",
      "episode: 261/500, score: 195, e: 0.28\n",
      "episode: 262/500, score: 165, e: 0.28\n",
      "episode: 263/500, score: 199, e: 0.27\n",
      "episode: 264/500, score: 231, e: 0.27\n",
      "episode: 265/500, score: 196, e: 0.27\n",
      "episode: 266/500, score: 176, e: 0.27\n",
      "episode: 267/500, score: 204, e: 0.27\n",
      "episode: 268/500, score: 192, e: 0.27\n",
      "episode: 269/500, score: 145, e: 0.27\n",
      "episode: 270/500, score: 302, e: 0.26\n",
      "episode: 271/500, score: 221, e: 0.26\n",
      "episode: 272/500, score: 182, e: 0.26\n",
      "episode: 273/500, score: 283, e: 0.26\n",
      "episode: 274/500, score: 246, e: 0.26\n",
      "episode: 275/500, score: 329, e: 0.26\n",
      "episode: 276/500, score: 295, e: 0.26\n",
      "episode: 277/500, score: 187, e: 0.26\n",
      "episode: 278/500, score: 339, e: 0.25\n",
      "episode: 279/500, score: 266, e: 0.25\n",
      "episode: 280/500, score: 189, e: 0.25\n",
      "episode: 281/500, score: 234, e: 0.25\n",
      "episode: 282/500, score: 479, e: 0.25\n",
      "episode: 283/500, score: 189, e: 0.25\n",
      "episode: 284/500, score: 200, e: 0.25\n",
      "episode: 285/500, score: 195, e: 0.25\n",
      "episode: 286/500, score: 322, e: 0.24\n",
      "episode: 287/500, score: 237, e: 0.24\n",
      "episode: 288/500, score: 246, e: 0.24\n",
      "episode: 289/500, score: 253, e: 0.24\n",
      "episode: 290/500, score: 176, e: 0.24\n",
      "episode: 291/500, score: 208, e: 0.24\n",
      "episode: 292/500, score: 183, e: 0.24\n",
      "episode: 293/500, score: 217, e: 0.24\n",
      "episode: 294/500, score: 223, e: 0.23\n",
      "episode: 295/500, score: 180, e: 0.23\n",
      "episode: 296/500, score: 184, e: 0.23\n",
      "episode: 297/500, score: 199, e: 0.23\n",
      "episode: 298/500, score: 208, e: 0.23\n",
      "episode: 299/500, score: 156, e: 0.23\n",
      "episode: 300/500, score: 218, e: 0.23\n",
      "episode: 301/500, score: 288, e: 0.23\n",
      "episode: 302/500, score: 189, e: 0.23\n",
      "episode: 303/500, score: 204, e: 0.22\n",
      "episode: 304/500, score: 205, e: 0.22\n",
      "episode: 305/500, score: 206, e: 0.22\n",
      "episode: 306/500, score: 30, e: 0.22\n",
      "episode: 307/500, score: 224, e: 0.22\n",
      "episode: 308/500, score: 153, e: 0.22\n",
      "episode: 309/500, score: 157, e: 0.22\n",
      "episode: 310/500, score: 153, e: 0.22\n",
      "episode: 311/500, score: 174, e: 0.22\n",
      "episode: 312/500, score: 183, e: 0.21\n",
      "episode: 313/500, score: 176, e: 0.21\n",
      "episode: 314/500, score: 173, e: 0.21\n",
      "episode: 315/500, score: 181, e: 0.21\n",
      "episode: 316/500, score: 157, e: 0.21\n",
      "episode: 317/500, score: 148, e: 0.21\n",
      "episode: 318/500, score: 134, e: 0.21\n",
      "episode: 319/500, score: 154, e: 0.21\n",
      "episode: 320/500, score: 182, e: 0.21\n",
      "episode: 321/500, score: 165, e: 0.21\n",
      "episode: 322/500, score: 224, e: 0.2\n",
      "episode: 323/500, score: 175, e: 0.2\n",
      "episode: 324/500, score: 193, e: 0.2\n",
      "episode: 325/500, score: 337, e: 0.2\n",
      "episode: 326/500, score: 245, e: 0.2\n",
      "episode: 327/500, score: 202, e: 0.2\n",
      "episode: 328/500, score: 186, e: 0.2\n",
      "episode: 329/500, score: 180, e: 0.2\n",
      "episode: 330/500, score: 231, e: 0.2\n",
      "episode: 331/500, score: 156, e: 0.2\n",
      "episode: 332/500, score: 327, e: 0.19\n",
      "episode: 333/500, score: 281, e: 0.19\n",
      "episode: 334/500, score: 247, e: 0.19\n",
      "episode: 335/500, score: 329, e: 0.19\n",
      "episode: 336/500, score: 231, e: 0.19\n",
      "episode: 337/500, score: 291, e: 0.19\n",
      "episode: 338/500, score: 245, e: 0.19\n",
      "episode: 339/500, score: 219, e: 0.19\n",
      "episode: 340/500, score: 173, e: 0.19\n",
      "episode: 341/500, score: 160, e: 0.19\n",
      "episode: 342/500, score: 171, e: 0.18\n",
      "episode: 343/500, score: 191, e: 0.18\n",
      "episode: 344/500, score: 184, e: 0.18\n",
      "episode: 345/500, score: 202, e: 0.18\n",
      "episode: 346/500, score: 161, e: 0.18\n",
      "episode: 347/500, score: 146, e: 0.18\n",
      "episode: 348/500, score: 188, e: 0.18\n",
      "episode: 349/500, score: 176, e: 0.18\n",
      "episode: 350/500, score: 231, e: 0.18\n",
      "episode: 351/500, score: 219, e: 0.18\n",
      "episode: 352/500, score: 226, e: 0.18\n",
      "episode: 353/500, score: 228, e: 0.17\n",
      "episode: 354/500, score: 233, e: 0.17\n",
      "episode: 355/500, score: 20, e: 0.17\n",
      "episode: 356/500, score: 189, e: 0.17\n",
      "episode: 357/500, score: 272, e: 0.17\n",
      "episode: 358/500, score: 261, e: 0.17\n",
      "episode: 359/500, score: 320, e: 0.17\n",
      "episode: 360/500, score: 190, e: 0.17\n",
      "episode: 361/500, score: 207, e: 0.17\n",
      "episode: 362/500, score: 333, e: 0.17\n",
      "episode: 363/500, score: 228, e: 0.17\n",
      "episode: 364/500, score: 247, e: 0.17\n",
      "episode: 365/500, score: 288, e: 0.16\n",
      "episode: 366/500, score: 228, e: 0.16\n",
      "episode: 367/500, score: 415, e: 0.16\n",
      "episode: 368/500, score: 288, e: 0.16\n",
      "episode: 369/500, score: 205, e: 0.16\n",
      "episode: 370/500, score: 218, e: 0.16\n",
      "episode: 371/500, score: 311, e: 0.16\n",
      "episode: 372/500, score: 172, e: 0.16\n",
      "episode: 373/500, score: 224, e: 0.16\n",
      "episode: 374/500, score: 267, e: 0.16\n",
      "episode: 375/500, score: 212, e: 0.16\n",
      "episode: 376/500, score: 197, e: 0.16\n",
      "episode: 377/500, score: 177, e: 0.15\n",
      "episode: 378/500, score: 499, e: 0.15\n",
      "episode: 379/500, score: 287, e: 0.15\n",
      "episode: 380/500, score: 300, e: 0.15\n",
      "episode: 381/500, score: 383, e: 0.15\n",
      "episode: 382/500, score: 359, e: 0.15\n",
      "episode: 383/500, score: 282, e: 0.15\n",
      "episode: 384/500, score: 221, e: 0.15\n",
      "episode: 385/500, score: 182, e: 0.15\n",
      "episode: 386/500, score: 174, e: 0.15\n",
      "episode: 387/500, score: 181, e: 0.15\n",
      "episode: 388/500, score: 202, e: 0.15\n",
      "episode: 389/500, score: 150, e: 0.15\n",
      "episode: 390/500, score: 223, e: 0.15\n",
      "episode: 391/500, score: 270, e: 0.14\n",
      "episode: 392/500, score: 189, e: 0.14\n",
      "episode: 393/500, score: 197, e: 0.14\n",
      "episode: 394/500, score: 272, e: 0.14\n",
      "episode: 395/500, score: 267, e: 0.14\n",
      "episode: 396/500, score: 388, e: 0.14\n",
      "episode: 397/500, score: 431, e: 0.14\n",
      "episode: 398/500, score: 253, e: 0.14\n",
      "episode: 399/500, score: 268, e: 0.14\n",
      "episode: 400/500, score: 499, e: 0.14\n",
      "episode: 401/500, score: 304, e: 0.14\n",
      "episode: 402/500, score: 219, e: 0.14\n",
      "episode: 403/500, score: 199, e: 0.14\n",
      "episode: 404/500, score: 234, e: 0.14\n",
      "episode: 405/500, score: 237, e: 0.13\n",
      "episode: 406/500, score: 249, e: 0.13\n",
      "episode: 407/500, score: 195, e: 0.13\n",
      "episode: 408/500, score: 225, e: 0.13\n",
      "episode: 409/500, score: 233, e: 0.13\n",
      "episode: 410/500, score: 225, e: 0.13\n",
      "episode: 411/500, score: 322, e: 0.13\n",
      "episode: 412/500, score: 202, e: 0.13\n",
      "episode: 413/500, score: 146, e: 0.13\n",
      "episode: 414/500, score: 234, e: 0.13\n",
      "episode: 415/500, score: 209, e: 0.13\n",
      "episode: 416/500, score: 218, e: 0.13\n",
      "episode: 417/500, score: 239, e: 0.13\n",
      "episode: 418/500, score: 176, e: 0.13\n",
      "episode: 419/500, score: 263, e: 0.13\n",
      "episode: 420/500, score: 174, e: 0.12\n",
      "episode: 421/500, score: 199, e: 0.12\n",
      "episode: 422/500, score: 242, e: 0.12\n",
      "episode: 423/500, score: 289, e: 0.12\n",
      "episode: 424/500, score: 330, e: 0.12\n",
      "episode: 425/500, score: 243, e: 0.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 426/500, score: 275, e: 0.12\n",
      "episode: 427/500, score: 251, e: 0.12\n",
      "episode: 428/500, score: 225, e: 0.12\n",
      "episode: 429/500, score: 499, e: 0.12\n",
      "episode: 430/500, score: 308, e: 0.12\n",
      "episode: 431/500, score: 213, e: 0.12\n",
      "episode: 432/500, score: 246, e: 0.12\n",
      "episode: 433/500, score: 302, e: 0.12\n",
      "episode: 434/500, score: 472, e: 0.12\n",
      "episode: 435/500, score: 402, e: 0.12\n",
      "episode: 436/500, score: 264, e: 0.12\n",
      "episode: 437/500, score: 221, e: 0.11\n",
      "episode: 438/500, score: 253, e: 0.11\n",
      "episode: 439/500, score: 207, e: 0.11\n",
      "episode: 440/500, score: 301, e: 0.11\n",
      "episode: 441/500, score: 233, e: 0.11\n",
      "episode: 442/500, score: 216, e: 0.11\n",
      "episode: 443/500, score: 232, e: 0.11\n",
      "episode: 444/500, score: 245, e: 0.11\n",
      "episode: 445/500, score: 172, e: 0.11\n",
      "episode: 446/500, score: 169, e: 0.11\n",
      "episode: 447/500, score: 15, e: 0.11\n",
      "episode: 448/500, score: 156, e: 0.11\n",
      "episode: 449/500, score: 160, e: 0.11\n",
      "episode: 450/500, score: 171, e: 0.11\n",
      "episode: 451/500, score: 152, e: 0.11\n",
      "episode: 452/500, score: 157, e: 0.11\n",
      "episode: 453/500, score: 239, e: 0.11\n",
      "episode: 454/500, score: 184, e: 0.11\n",
      "episode: 455/500, score: 178, e: 0.1\n",
      "episode: 456/500, score: 175, e: 0.1\n",
      "episode: 457/500, score: 191, e: 0.1\n",
      "episode: 458/500, score: 304, e: 0.1\n",
      "episode: 459/500, score: 323, e: 0.1\n",
      "episode: 460/500, score: 381, e: 0.1\n",
      "episode: 461/500, score: 361, e: 0.1\n",
      "episode: 462/500, score: 375, e: 0.1\n",
      "episode: 463/500, score: 218, e: 0.1\n",
      "episode: 464/500, score: 225, e: 0.1\n",
      "episode: 465/500, score: 181, e: 0.1\n",
      "episode: 466/500, score: 192, e: 0.099\n",
      "episode: 467/500, score: 247, e: 0.099\n",
      "episode: 468/500, score: 173, e: 0.098\n",
      "episode: 469/500, score: 193, e: 0.098\n",
      "episode: 470/500, score: 191, e: 0.097\n",
      "episode: 471/500, score: 200, e: 0.097\n",
      "episode: 472/500, score: 264, e: 0.096\n",
      "episode: 473/500, score: 182, e: 0.096\n",
      "episode: 474/500, score: 302, e: 0.095\n",
      "episode: 475/500, score: 354, e: 0.095\n",
      "episode: 476/500, score: 499, e: 0.094\n",
      "episode: 477/500, score: 499, e: 0.094\n",
      "episode: 478/500, score: 316, e: 0.093\n",
      "episode: 479/500, score: 273, e: 0.093\n",
      "episode: 480/500, score: 167, e: 0.092\n",
      "episode: 481/500, score: 224, e: 0.092\n",
      "episode: 482/500, score: 219, e: 0.092\n",
      "episode: 483/500, score: 222, e: 0.091\n",
      "episode: 484/500, score: 248, e: 0.091\n",
      "episode: 485/500, score: 209, e: 0.09\n",
      "episode: 486/500, score: 235, e: 0.09\n",
      "episode: 487/500, score: 222, e: 0.089\n",
      "episode: 488/500, score: 257, e: 0.089\n",
      "episode: 489/500, score: 218, e: 0.088\n",
      "episode: 490/500, score: 210, e: 0.088\n",
      "episode: 491/500, score: 189, e: 0.088\n",
      "episode: 492/500, score: 220, e: 0.087\n",
      "episode: 493/500, score: 213, e: 0.087\n",
      "episode: 494/500, score: 193, e: 0.086\n",
      "episode: 495/500, score: 212, e: 0.086\n",
      "episode: 496/500, score: 225, e: 0.085\n",
      "episode: 497/500, score: 188, e: 0.085\n",
      "episode: 498/500, score: 201, e: 0.084\n",
      "episode: 499/500, score: 199, e: 0.084\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "batch_size = 100    # tamaño del batch con el que se re-entrena el modelo neuronal\n",
    "EPISODES = 500     # numero de episodios\n",
    "\n",
    "env = gym.make('CartPole-v1')   # carga el modelo Cartpole de gym\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)  # instancia el agente deep q-network\n",
    "\n",
    "for e in range(EPISODES):   # por cada episodio\n",
    "    state = env.reset()     # resetea el entorno\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for step in range(500):   # se prueba el agente hasta 500 pasos, sale de este loop cuando se cae en estado terminal (perdida de equilibrio)  \n",
    "        #env.render()  # renderiza el entorno (no funciona en Colab)\n",
    "        action = agent.get_action(state)   # obtiene una accion del agente\n",
    "        next_state, reward, done, _ = env.step(action)  # ejecuta action en el entorno y obtiene: nuevo estado, reward y flag done (si es estado terminal)\n",
    "        reward = reward if not done else -10  # si es estado terminal el reward es -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done) # almacena esta experiencia en la memoria del agente\n",
    "        state = next_state   # actualiza el estado actual al nuevo estado\n",
    "        if done:  # si es estado terminal, imprime resultados del trial. El escore del trial es el numero de pasos que logro ejecutar el agente\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, EPISODES, step, agent.epsilon))\n",
    "            break\n",
    "    if len(agent.memory) > batch_size:  # si el agente tiene suficiente experiencias en su memoria -> ajusta su modelo neuronal \n",
    "        agent.replay(batch_size)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion para probar el agente entrenado  en el entorno CartPole\n",
    "\n",
    "play_CartPole()  no realiza entrenamiento, solo actua en el ambiente de acuerdo a su modelo aprendido previamente\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_CartPole(agent, trials = 1):\n",
    "    env = gym.make('CartPole-v1')  \n",
    "    scores = []\n",
    "    for trial in range(trials):\n",
    "        score = 0\n",
    "        game_memory = []\n",
    "        state = []\n",
    "        env.reset()\n",
    "        for step in range(500): # en cada trial ejecuta 500  pasos\n",
    "            env.render()\n",
    "\n",
    "            if len(state) == 0:    # si es el primer movimiento  -> escoge una accion aleatoria\n",
    "                action = random.randrange(0,2)\n",
    "            else:\n",
    "                action_values = agent.model.predict(state.reshape(1, 4)) # predice los q valores con la RN del agente\n",
    "                action = np.argmax(action_values[0])      # retorna la accion con el maximo q-valor predicho\n",
    "\n",
    "            next_state, reward, done, _  = env.step(action)  # corre el entorno un step ejecutando la accion inferida\n",
    "            score += reward   # acumula el reward (reward=1 en cualquier estado no terminal)\n",
    "            state = next_state\n",
    "            game_memory.append([next_state, action])\n",
    "            if done: \n",
    "                print(\"Play {}/{}, score: {}\".format(trial, trials, step))\n",
    "                break\n",
    "        scores.append(score)\n",
    "    env.close()\n",
    " \n",
    "    print(\"Score medio = {}\".format(sum(scores) /float(trials)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play 0/5, score: 189\n",
      "Play 1/5, score: 221\n",
      "Play 2/5, score: 216\n",
      "Play 3/5, score: 207\n",
      "Play 4/5, score: 221\n",
      "Score medio = 211.8\n"
     ]
    }
   ],
   "source": [
    "# prueba el agente 5 trials del entorno\n",
    "play_CartPole(agent,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
